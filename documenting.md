
## ‚öôÔ∏è Phase 1: Infrastructure Setup with Terraform

In this phase, I created the base setup for my Serverless Remediation Lab. The goal was to simulate an insecure S3 bucket, and prepare the automation stack (Lambda + S3 Trigger) to respond to misconfigurations like public access.

---

### üîπ Step 1: Project Folder Structure

I created a new folder and added Terraform config files along with a folder for the Lambda script.

```bash
mkdir serverless-remediation-lab
cd serverless-remediation-lab
touch main.tf variables.tf outputs.tf
mkdir lambda
```

> üì∏ Screenshot: Project folder structure showing `main.tf`, `variables.tf`, `outputs.tf`, and `lambda/` folder
<img width="1116" height="798" alt="VirtualBox_Kali Linux_19_07_2025_23_33_40" src="https://github.com/user-attachments/assets/7177bb9c-0dba-4828-8b3d-a1fe055bcdab" />

---
Below is the main infrastructure definition, broken into IAM, Lambda, S3 bucket, and event trigger blocks.
### üîπ Step 2: Writing Terraform Configuration (main.tf)

I used `main.tf` to define:
- AWS provider
- S3 bucket (intentionally misconfigured)
- Lambda + S3 bucket + permissions and trigger

```hcl
provider "aws" {
  region = "us-east-1"
}

# Create S3 bucket
resource "aws_s3_bucket" "target_bucket" {
  bucket        = "jmcoded-insecure-public-bucket-001"
  force_destroy = true # Be careful with this in production!
}

# Disable Block Public Access for the bucket (intentionally vulnerable for the lab)
resource "aws_s3_bucket_public_access_block" "public_access" {
  bucket = aws_s3_bucket.target_bucket.id

  block_public_acls       = false
  block_public_policy     = false
  ignore_public_acls      = false
  restrict_public_buckets = false
}

# Set ownership controls (optional but common)
resource "aws_s3_bucket_ownership_controls" "ownership" {
  bucket = aws_s3_bucket.target_bucket.id

  rule {
    object_ownership = "BucketOwnerPreferred"
  }
}

# Make bucket publicly readable via policy (this is what the Lambda will remediate)
resource "aws_s3_bucket_policy" "public_read_policy" {
  bucket = aws_s3_bucket.target_bucket.id

  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Sid       = "PublicReadGetObject",
        Effect    = "Allow",
        Principal = "*",
        Action    = "s3:GetObject",
        Resource  = "${aws_s3_bucket.target_bucket.arn}/*"
      }
    ]
  })
}

# IAM Role for Lambda function execution
resource "aws_iam_role" "lambda_exec_role" {
  name = "lambda_exec_role_jmcoded"

  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Principal = {
          Service = "lambda.amazonaws.com"
        },
        Action = "sts:AssumeRole"
      }
    ]
  })
}

# Attach basic execution policy to the role (for CloudWatch Logs)
resource "aws_iam_role_policy_attachment" "lambda_basic_policy" {
  role       = aws_iam_role.lambda_exec_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
}

# Custom IAM Policy for Lambda to perform S3 remediation actions
resource "aws_iam_role_policy" "lambda_s3_remediation_policy" {
  name = "lambda_s3_remediation_policy_jmcoded"
  role = aws_iam_role.lambda_exec_role.id

  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Action = [
          "s3:PutObjectAcl",     # Allows Lambda to change object ACLs
          "s3:GetObjectAcl",     # Allows Lambda to read object ACLs (useful for context)
          "s3:GetBucketPolicy",  # Allows Lambda to read bucket policies
          "s3:PutBucketPolicy",  # Allows Lambda to modify bucket policies
          "s3:DeleteBucketPolicy", # Allows Lambda to delete bucket policies
          "logs:CreateLogGroup", # Required for Lambda logging
          "logs:CreateLogStream", # Required for Lambda logging
          "logs:PutLogEvents"    # Required for Lambda logging
        ],
        Resource = "*" # For lab simplicity, but scope down in production!
      }
    ]
  })
}

# AWS Lambda Function definition
resource "aws_lambda_function" "remediation_lambda" {
  function_name    = "RemediateS3PublicAccess"
  filename         = "lambda_function_payload.zip"
  handler          = "lambda_function.lambda_handler"
  runtime          = "python3.9"
  role             = aws_iam_role.lambda_exec_role.arn
  source_code_hash = filebase64sha256("lambda_function_payload.zip")
  timeout          = 30 # Increased timeout to prevent previous errors
}

# Grant S3 permission to invoke the Lambda function
resource "aws_lambda_permission" "allow_s3_to_call_remediation_lambda" {
  statement_id  = "AllowExecutionFromS3Bucket"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.remediation_lambda.function_name
  principal     = "s3.amazonaws.com"
  source_arn    = aws_s3_bucket.target_bucket.arn
}

# Configure S3 to send object creation events to the Lambda
resource "aws_s3_bucket_notification" "target_bucket_notification" {
  bucket = aws_s3_bucket.target_bucket.id

  lambda_function {
    lambda_function_arn = aws_lambda_function.remediation_lambda.arn
    events              = ["s3:ObjectCreated:*"] # Trigger on any new object creation
  }

  #  permission for notification is set up.
  depends_on = [aws_lambda_permission.allow_s3_to_call_remediation_lambda]
}

```

> üì∏ Screenshot: VS Code showing full `main.tf` with provider, S3, and Lambda trigger
<img width="1116" height="798" alt="VirtualBox_Kali Linux_20_07_2025_23_52_50" src="https://github.com/user-attachments/assets/dbf217de-bdc5-46dd-b88a-7ad518c221f1" />

---
### üìÑStep 3: `variables.tf` ‚Äî Terraform Variables for Deployment

This file defines key variables used in the deployment process. By setting them here, we keep the configuration clean and reusable.

```hcl
variable "aws_region" {
  description = "The AWS region to deploy to"
  type        = string
  default     = "us-east-1"
}

variable "lambda_function_name" {
  description = "The name of the Lambda function"
  type        = string
  default     = "remediate-public-s3"
}

variable "iam_role_name" {
  description = "The name of the IAM role for Lambda execution"
  type        = string
  default     = "lambda-s3-remediation-role"
}
variable "target_bucket_name" {
  description = "The name of the insecure bucket"
  type        = string
}

```
<img width="1116" height="798" alt="VirtualBox_Kali Linux_20_07_2025_00_00_45" src="https://github.com/user-attachments/assets/fce8dff9-646d-4c38-a0ff-1aeb81113f65" />

üîç **Breakdown**

- `aws_region`: Specifies the AWS region where Terraform will deploy all resources.
- `lambda_function_name`: The name of the Lambda function that will run the S3 remediation logic.
- `iam_role_name`: The IAM role that gives the Lambda function permission to act (like checking/modifying S3 buckets).

### üìÑ `outputs.tf` ‚Äî Terraform Output Values

This file is used to print key information after deployment. It helps confirm that everything worked and shows important resource names or ARNs.

```hcl
output "lambda_function_name" {
  value = aws_lambda_function.remediation_lambda.function_name
}

output "lambda_function_arn" {
  value = aws_lambda_function.remediation_lambda.arn
}


```
<img width="1116" height="798" alt="VirtualBox_Kali Linux_20_07_2025_00_12_47" src="https://github.com/user-attachments/assets/05d56ad5-f6a2-4a44-82b0-a8b2fc0b99ff" />

üîç **Breakdown**

- `lambda_function_name`: Displays the actual name of the Lambda function after deployment.
- `lambda_function_arn`: Shows the full Amazon Resource Name (ARN), which uniquely identifies the Lambda function across AWS.

üß†  These outputs make it easy to grab values for monitoring, logging, or future integrations.
Then add a short section for terraform.tfvars:

### üìÑ `terraform.tfvars` ‚Äî Values Injected at Deploy Time

I created a terraform.tfvars file to pass the actual S3 bucket name into the configuration during deployment.
```
target_bucket_name = "my-insecure-public-bucket"
```
<img width="1116" height="798" alt="VirtualBox_Kali Linux_20_07_2025_00_35_07" src="https://github.com/user-attachments/assets/53dd015c-5420-486f-83b9-32ebdba0bc7a" />

üß† This setup keeps the `main.tf` clean and lets us update values easily without touching the main configuration.

## ‚öôÔ∏è Phase 1.5: Creating and Zipping the Lambda Remediation Bot

Before running `terraform apply`, I needed to create the actual Python bot that Terraform will deploy as an AWS Lambda function. Terraform is expecting a file called `lambda.zip`, which will contain this logic.

---

### üßæ Step 1: Create the Lambda Script (`lambda_function.py`)

I started by creating the file:

```bash
nano lambda_function.py
```

Then I pasted the following Python code:

```python
import boto3
import json
import os

s3 = boto3.client('s3')

def lambda_handler(event, context):
    print("Received event: " + json.dumps(event))

    try: # Main try block starts here
        for record in event['Records']:
            bucket = record['s3']['bucket']['name']
            key = record['s3']['object']['key']
            event_name = record['eventName']
            
            print(f"Processing bucket: {bucket}, key: {key}, event: {event_name}")

            # --- Remediation Attempt 1: Set Object ACL to Private ---
            try: # Inner try block for ACL starts here
                s3.put_object_acl(Bucket=bucket, Key=key, ACL='private')
                print(f"Successfully set object ACL for {key} in {bucket} to 'private'.")
            except Exception as e: # This is the line 69, correctly indented under the inner try
                print(f"Error setting object ACL for {bucket}/{key}: {e}")

            # --- Remediation Attempt 2: Check and Remove Public Bucket Policy Statement ---
            try: # Inner try block for Policy starts here
                current_policy = {}
                try: # Nested try for getting policy
                    policy_response = s3.get_bucket_policy(Bucket=bucket)
                    current_policy = json.loads(policy_response['Policy'])
                    print(f"Original bucket policy for {bucket}: {json.dumps(current_policy, indent=2)}")
                except s3.exceptions.ClientError as e:
                    if e.response['Error']['Code'] == 'NoSuchBucketPolicy':
                        print(f"No bucket policy found for {bucket}. No policy remediation needed.")
                        continue # Move to next record if no policy
                    else:
                        raise e # Re-raise if it's another type of error

                # Filter out statements that grant public GetObject access
                updated_statements = []
                policy_was_public_get_object = False
                for statement in current_policy.get('Statement', []):
                    if (statement.get('Effect') == 'Allow' and
                        statement.get('Principal') == '*' and
                        ('s3:GetObject' in statement.get('Action') or (isinstance(statement.get('Action'), list) and 's3:GetObject' in statement.get('Action'))) and
                        statement.get('Resource') == f"arn:aws:s3:::{bucket}/*"):
                        
                        print(f"Found and removing public GetObject policy statement: {statement}")
                        policy_was_public_get_object = True
                    else:
                        updated_statements.append(statement) # Keep non-public statements

                if policy_was_public_get_object:
                    if not updated_statements: # If all statements were public and removed
                        print(f"Bucket policy for {bucket} is now empty after removing public statements. Deleting policy.")
                        s3.delete_bucket_policy(Bucket=bucket)
                    else: # If some statements remain, update the policy
                        updated_policy = current_policy.copy()
                        updated_policy['Statement'] = updated_statements
                        print(f"Updating bucket policy for {bucket}: {json.dumps(updated_policy, indent=2)}")
                        s3.put_bucket_policy(Bucket=bucket, Policy=json.dumps(updated_policy))
                else:
                    print(f"No public GetObject statement found in bucket policy for {bucket}.")

            except Exception as e: # This should be correctly indented under its 'try' block
                print(f"Error during bucket policy remediation for {bucket}: {e}")
                raise e # Re-raise the exception to indicate a failure

    except Exception as e: # This 'except' is for the main 'try' block
        print(f"A critical error occurred in the Lambda handler: {e}")
        raise e # Re-raise the exception to indicate a failure

    print("Remediation process completed.")
    return {
        'statusCode': 200,
        'body': 'S3 Public Access remediation executed.'
    }
```

‚úÖ After saving and exiting (`CTRL + O`, `Enter`, then `CTRL + X`), I moved on to zip the file.
<img width="1116" height="798" alt="VirtualBox_Kali Linux_20_07_2025_23_41_56" src="https://github.com/user-attachments/assets/edc74c45-87df-4831-a421-3c7ba5c436e0" />

---

### üß∞ Step 2: Zip the Lambda Script

I zipped the Python file so Terraform can upload it to AWS:

```bash
zip lambda.zip lambda_function.py
```

This generated the `lambda.zip` archive. This file is referenced inside the Terraform config:

```hcl
source_code_hash = filebase64sha256("lambda.zip")
```

---

‚úÖ With the bot zipped and ready, I ran:

```bash
terraform plan
```

and then:

```bash
terraform apply
```

No more errors ‚Äî Lambda was deployed correctly!

### üîπ Step 4: Terraform Init & Apply

Once the files were saved, I initialized and deployed the infrastructure:

```bash
terraform init
terraform plan
terraform apply
```

> üì∏ Screenshot: `terraform init` and `terraform apply` running successfully in terminal  
<img width="1116" height="798" alt="VirtualBox_Kali Linux_20_07_2025_03_14_50" src="https://github.com/user-attachments/assets/dd0a43df-5a23-48b2-8689-130511e79fb1" />

---

‚úÖ **Phase 1 Complete ‚Äî S3 bucket and Lambda trigger are live. Next, I‚Äôll configure the Lambda (`remediate.py`) that listens for public access and auto-remediates it.
---

## ‚úÖ Phase 1 Recap: Misconfigured S3 Bucket + Lambda Remediation Setup

In this phase, I used Terraform to deploy a misconfigured S3 bucket and a Lambda-based remediation function.

### üîß What I Set Up

- A vulnerable S3 bucket named `jmcoded-insecure-public-bucket-001` with **public-read access** via a bucket policy.
- A Lambda function (`remediate_s3`) that is designed to trigger automatically when a new object is uploaded.
- An IAM execution role with the necessary permissions for the Lambda function.
- All infrastructure was deployed using Terraform with clean modular files:
  - `main.tf`
  - `variables.tf`
  - `outputs.tf`
  - `terraform.tfvars`

### üìå Key Objective

The goal here was to simulate a common S3 misconfiguration ‚Äî exposing objects to the public ‚Äî and set up an **automatic response** using AWS Lambda.

No actual uploads or testing were done yet. That comes in the next phase.

---

‚úÖ Setup complete. The misconfigured bucket and Lambda function are now live. Next step: simulate attacker behavior and confirm Lambda auto-remediation works as expected.

---
## ‚öîÔ∏è Phase 2: Simulating the Attack and Testing Auto-Remediation

Now that the S3 bucket, Lambda function, and CloudTrail are all properly deployed using Terraform, it‚Äôs time to simulate a real-world misconfiguration scenario. The goal of this phase is to test whether the Lambda function correctly detects public file uploads and revokes the `public-read` access automatically.

---

### üìÅ Step 1: Upload a Public File to the Bucket

I created a simple test file (`payload.txt`) and uploaded it to the insecure bucket with public access enabled:

```bash
echo "Sensitive info inside" > payload.txt

aws s3 cp payload.txt s3://jmcoded-insecure-public-bucket-001/payload.txt --acl public-read
```
<img width="1116" height="798" alt="VirtualBox_Kali Linux_20_07_2025_03_49_41" src="https://github.com/user-attachments/assets/2c306ed9-bfd7-4e16-b234-d8e1778914bb" />

This simulates a real-world mistake where someone uploads sensitive data to a public-facing S3 bucket ‚Äî exactly the type of event we want to catch and remediate.

---

### üìú Step 2: Monitor CloudTrail for Event Logs

After uploading the file, I opened the AWS Console and GuardDuty automatically detected and alerted the S3 misconfiguration as a potential breach.

### üö® Detection Summary

- **Tool**: Amazon GuardDuty
- **Finding**: Public Anonymous Access Granted to S3 Bucket
- **Bucket Name**: `jmcoded-insecure-public-bucket-001`
- **Detection Type**: `Policy:S3/BucketAnonymousAccessGranted`
- **User Responsible**: `AdminUser`
- **API Call**: `PutBucketPolicy`
- **Time**: `07-20-2025 03:21 UTC`
- **Public IP**: `102.88.107.71 (MTN Nigeria)`
- **Region**: `us-east-1`
- **Severity**: HIGH

### üñºÔ∏è Screenshot
<img width="960" height="505" alt="Screenshot 2025-07-20 041206" src="https://github.com/user-attachments/assets/ee5f4ad0-17f3-4974-895a-aa01285bbbaa" />
---

> ‚úÖ This validates that the misconfigured S3 bucket was correctly detected by AWS native threat detection. We use this as proof of breach detection in this lab.
‚úÖ **Confirmed** that the file upload was logged properly, including the `public-read` permission.

---
---

## üîßDeploying the Auto-Remediation Lambda Function

In this phase, I created a Python-based Lambda function to automatically remediate public S3 bucket ACL changes. The goal is to detect when a bucket is made public and immediately revert the permission.

### üöÄ Deployment with Terraform

I added the Lambda function and bucket policy to the same `main.tf` and redeployed using `terraform apply`.
```bash
resource "aws_lambda_function" "remediation_lambda" {
  function_name    = "RemediateS3PublicAccess"
  filename         = "lambda_function_payload.zip"
  handler          = "lambda_function.lambda_handler"
  runtime          = "python3.9"
  role             = aws_iam_role.lambda_exec_role.arn
  source_code_hash = filebase64sha256("lambda_function_payload.zip")
}
```
<img width="1116" height="798" alt="VirtualBox_Kali Linux_20_07_2025_19_44_18" src="https://github.com/user-attachments/assets/38374857-dd2c-4960-834a-ce938bd84192" />

```bash
Plan: 2 to add, 0 to change, 0 to destroy.

aws_s3_bucket_policy.public_read_policy: Creation complete
aws_lambda_function.remediation_lambda: Creation complete
Apply complete! Resources: 2 added, 0 changed, 0 destroyed.
```

üì∏ _Screenshot: Terraform deployment result_
<img width="1116" height="798" alt="VirtualBox_Kali Linux_20_07_2025_19_43_39" src="https://github.com/user-attachments/assets/99399f7d-a7a9-4a4e-8558-ef96049b0e84" />

### üîç Confirming Lambda Deployment

After deployment, I verified on the AWS Console that the Lambda function was created correctly with:

- Name: `RemediateS3PublicAccess`
- Runtime: Python 3.9
- Handler: `lambda_function.lambda_handler`

üì∏ _Screenshot: Lambda function overview_
<img width="1920" height="1010" alt="image" src="https://github.com/user-attachments/assets/a59501c3-98d5-46ed-a0b3-419421933732" />

---


### ‚öôÔ∏è Step 3: Lambda Trigger & Auto-Remediation

Next, I checked the **CloudWatch Logs** under `/aws/lambda/<lambda_function_name>`.

üìå The Lambda function got triggered automatically right after the `PutObject` event.

In the logs, I saw output indicating it had detected the public-read ACL and successfully removed it using the `PutObjectAcl` API call.
<img width="1920" height="1080" alt="image" src="https://github.com/user-attachments/assets/8ca1e7d1-d204-4901-9d55-2c245fbdeab5" />

---

### ‚ùå Step 4: Verify That Public Access Was Removed

To verify the remediation, I tried accessing the file from a public browser link:

```
https://jmcoded-insecure-public-bucket-001.s3.amazonaws.com/payload.txt
```

Instead of showing the file, I got:

```
AccessDenied
```

üíØ That confirmed the Lambda function worked exactly as intended ‚Äî it revoked the public-read access automatically without manual intervention.
<img width="1920" height="1010" alt="image" src="https://github.com/user-attachments/assets/5ee7d422-d75a-4e10-8dc7-ea2d586e404e" />


### ‚úÖ Phase 2 Complete!

At this point, I‚Äôve fully simulated the insecure upload and verified that the **auto-remediation pipeline works from end to end**. The Lambda function successfully detected the violation and revoked the public permissions on its own.

## ‚úÖ Phase 3: Public Access Remediation Alerts via Amazon SNS

After confirming that the Lambda function successfully detects and removes public access to the S3 bucket, I set up **SNS (Simple Notification Service)** to send email alerts anytime a remediation happens.

---

### üîß Step 1: Create SNS Topic

I created a dedicated SNS topic to handle these alerts:

- **Topic name:** `S3PublicAccessRemediationAlerts-jmcoded`

This topic will be triggered from inside the Lambda function anytime it modifies ACLs or bucket policy.

---

### üìß Step 2: Subscribe My Email to SNS

To receive alerts, I subscribed my personal email to this SNS topic:
<img width="1116" height="798" alt="VirtualBox_Kali Linux_21_07_2025_01_25_14" src="https://github.com/user-attachments/assets/22437d4b-199a-41a6-838c-303dc8d83501" />

```bash
aws sns subscribe \
  --topic-arn arn:aws:sns:us-east-1:xxxxxxxxxxxx:S3PublicAccessRemediationAlerts-jmcoded \
  --protocol email \
  --notification-endpoint johnsonmatthewayobami@gmail.com
```

I confirmed the email from my inbox to complete the subscription.
<img width="1920" height="1010" alt="image" src="https://github.com/user-attachments/assets/2af881b4-92fb-4a62-8020-089504dce7f1" />
And then i added my lambda sns code

```bash
# Add at the top of lambda_function.py if not there
import boto3

# Add this after remediation blocks (e.g., after policy update) in lambda_handler
if SNS_TOPIC_ARN and remediated_targets:
    message = f"Bucket: {bucket}\nRemediated: {', '.join(remediated_targets)}\nStatus: Public access removed"
    sns.publish(TopicArn=SNS_TOPIC_ARN, Subject=f"S3 Alert - {bucket}", Message=message)
    print(f"Alert sent for {bucket}")

# Add to aws_lambda_function in main.tf
environment {
  variables = {
    SNS_TOPIC_ARN = "arn:aws:sns:us-east-1:xxxxxxxxxxxx:S3PublicAccessRemediationAlerts-jmcoded"  # Replace with your ARN
  }
}

```
# Screenshot: <img width="1920" height="909" alt="VirtualBox_Kali Linux_21_07_2025_01_48_24" src="https://github.com/user-attachments/assets/1ff52333-d1ba-4b44-88ea-9cbbbe5d556c" />

---

### üîÅ Step 3: Trigger the Flow Again

To test the full detection-remediation-alert pipeline:
<img width="1920" height="909" alt="VirtualBox_Kali Linux_21_07_2025_01_27_56" src="https://github.com/user-attachments/assets/87af03af-1e59-4b53-88ff-34d28c978699" />

- I uploaded `payload.txt` to the S3 bucket with **public-read** ACL  
- The Lambda was triggered automatically  
- It detected the public ACL and removed it  
- Then it also removed the overly permissive bucket policy  
- Finally, it published a notification to my SNS topic  

---

### üì® Final SNS Alert Email Received

I got this exact SNS alert in my inbox, confirming the remediation:

```
Bucket: jmcoded-insecure-public-bucket-001
Objects/Policies Remediated: Object ACL: payload.txt, Bucket Policy: jmcoded-insecure-public-bucket-001
Status: Public access removed. Please verify.
Timestamp: 2025-07-20T23:58:02.205Z
```
<img width="1920" height="1010" alt="image" src="https://github.com/user-attachments/assets/537447f7-f97d-4cc1-b1df-cbffda1d2829" />

---
## üß† Lab Conclusion: AWS S3 Misconfiguration Detection & Auto-Remediation

This lab was built to simulate a **realistic AWS S3 data breach scenario** ‚Äî from intentional public access misconfigurations to full auto-remediation using Lambda and alerting via SNS.

---

### üîê What I Built:

- **S3 Bucket Misconfiguration:** Simulated a public-access vulnerability by uploading a sensitive file (`payload.txt`) with `public-read` ACL and a permissive bucket policy.
- **CloudTrail Monitoring:** Set up CloudTrail to track S3 PutObject and PutBucketPolicy actions.
- **CloudWatch Event Rules:** Used EventBridge (CloudWatch Events) to detect public access grants in real time.
- **Lambda Remediation Function:** Built a Python Lambda function that:
  - Removes `public-read` ACLs
  - Deletes insecure bucket policies
  - Publishes alerts to an SNS topic
- **SNS Email Notification:** Configured SNS to notify me via email anytime a remediation occurs.

---

### üìå Skills Demonstrated:

- AWS S3 Security & IAM Concepts  
- EventBridge Rule Creation  
- Python Lambda Development  
- JSON Policy Analysis & Cleanup  
- SNS Configuration & Email Alerts  
- End-to-End Automation Without EC2

---
### üí≠ Final Thoughts:

This project helped me understand how serverless AWS components can be used together to build an **automated security remediation workflow**. 
---

